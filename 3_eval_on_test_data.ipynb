{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b411fe2",
   "metadata": {},
   "source": [
    "This notebook performs comprehensive RAG (Retrieval-Augmented Generation) evaluation using:\n",
    "- RAG setup from `0_rag_setup.ipynb`\n",
    "- Test data from `2_generate_test_data.ipynb`\n",
    "\n",
    "The following metrics are computed and summarized:\n",
    "1. **Context Precision**\n",
    "2. **Context Recall**\n",
    "3. **Response Relevance**\n",
    "4. **Faithfulness**\n",
    "5. **Factual Correctness**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5aef6564",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import (\n",
    "    compute_context_precision_score,\n",
    "    compute_context_recall_score,\n",
    "    compute_response_relevance_score,\n",
    "    compute_faithfulness_score,\n",
    "    compute_factual_correctness_score,\n",
    ")\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "from ragas.llms.base import llm_factory\n",
    "from IPython.display import Markdown, display\n",
    "import ast\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ada71cbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(\"configs.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abd2a2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODELS\n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "LLM_MODEL = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3d902f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize LLMs and embedding models\n",
    "llm_as_judge = llm_factory(LLM_MODEL)\n",
    "embedding_model = OpenAIEmbeddings(model=EMBEDDING_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edd26411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "TEST_DATA_PATH = \"generated_test_data/test_data.csv\"\n",
    "RESPONSE_DATA_DIR = \"generated_responses\"\n",
    "os.makedirs(RESPONSE_DATA_DIR, exist_ok=True)\n",
    "RESPONSE_DATA_PATH = os.path.join(RESPONSE_DATA_DIR, \"rag_responses_v1.csv\")\n",
    "VECTOR_DB_DIR = \"vector_db\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21880637",
   "metadata": {},
   "source": [
    "Read generated test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04ff8c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the generated test data\n",
    "test_data_df = pd.read_csv(TEST_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "939fd57a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63c7f246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference</th>\n",
       "      <th>synthesizer_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What was the author's experience upon arriving...</td>\n",
       "      <td>['3 May. Bistritz._--Left Munich at 8:35 P. M....</td>\n",
       "      <td>The author arrived in Vienna early the next mo...</td>\n",
       "      <td>single_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wot kind of experiences can a traveler expect ...</td>\n",
       "      <td>['such as we see in old missals; sometimes we ...</td>\n",
       "      <td>In London, a traveler can expect to encounter ...</td>\n",
       "      <td>single_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What does the Count's letter indicate about th...</td>\n",
       "      <td>['4 May._--I found that my landlord had got a ...</td>\n",
       "      <td>The Count's letter directed the narrator's lan...</td>\n",
       "      <td>single_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In the context of travel narratives, how is th...</td>\n",
       "      <td>['5 May. The Castle._--The grey of the morning...</td>\n",
       "      <td>The term 'vrolok' is significant as it represe...</td>\n",
       "      <td>single_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the significance of the Mittel Land in...</td>\n",
       "      <td>['the Hospadars would not repair them, lest th...</td>\n",
       "      <td>The Mittel Land is described as a beautiful re...</td>\n",
       "      <td>single_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0  What was the author's experience upon arriving...   \n",
       "1  Wot kind of experiences can a traveler expect ...   \n",
       "2  What does the Count's letter indicate about th...   \n",
       "3  In the context of travel narratives, how is th...   \n",
       "4  What is the significance of the Mittel Land in...   \n",
       "\n",
       "                                  reference_contexts  \\\n",
       "0  ['3 May. Bistritz._--Left Munich at 8:35 P. M....   \n",
       "1  ['such as we see in old missals; sometimes we ...   \n",
       "2  ['4 May._--I found that my landlord had got a ...   \n",
       "3  ['5 May. The Castle._--The grey of the morning...   \n",
       "4  ['the Hospadars would not repair them, lest th...   \n",
       "\n",
       "                                           reference  \\\n",
       "0  The author arrived in Vienna early the next mo...   \n",
       "1  In London, a traveler can expect to encounter ...   \n",
       "2  The Count's letter directed the narrator's lan...   \n",
       "3  The term 'vrolok' is significant as it represe...   \n",
       "4  The Mittel Land is described as a beautiful re...   \n",
       "\n",
       "                        synthesizer_name  \n",
       "0  single_hop_specific_query_synthesizer  \n",
       "1  single_hop_specific_query_synthesizer  \n",
       "2  single_hop_specific_query_synthesizer  \n",
       "3  single_hop_specific_query_synthesizer  \n",
       "4  single_hop_specific_query_synthesizer  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b35af81",
   "metadata": {},
   "source": [
    "Initialize vector database retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fd16a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load persistent vector DB ===\n",
    "vectordb = Chroma(\n",
    "    persist_directory=VECTOR_DB_DIR,\n",
    "    embedding_function=OpenAIEmbeddings(model=EMBEDDING_MODEL),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03aae2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the LLM\n",
    "llm = ChatOpenAI(model=LLM_MODEL, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77ef5cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    return_source_documents=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78de8689",
   "metadata": {},
   "source": [
    "Generate responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd8c5d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_obj_arr = []\n",
    "for _, row in test_data_df.iterrows():\n",
    "    response = qa_chain.invoke(row[\"user_input\"])\n",
    "    response_obj_arr.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5befefd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save responses to new dataframe using response_obj_arr\n",
    "response_df = pd.DataFrame(\n",
    "    {\n",
    "        \"query\": [resp[\"query\"] for resp in response_obj_arr],\n",
    "        \"result\": [resp[\"result\"] for resp in response_obj_arr],\n",
    "        \"source_documents_names_arr\": [\n",
    "            [doc.metadata[\"source\"] for doc in resp[\"source_documents\"]]\n",
    "            for resp in response_obj_arr\n",
    "        ],\n",
    "        \"source_documents_contents_arr\": [\n",
    "            [doc.page_content for doc in resp[\"source_documents\"]]\n",
    "            for resp in response_obj_arr\n",
    "        ],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3b68966",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_df.to_csv(RESPONSE_DATA_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02bc21a",
   "metadata": {},
   "source": [
    "Read saved responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e80f0c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_df = pd.read_csv(RESPONSE_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b599daea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>result</th>\n",
       "      <th>source_documents_names_arr</th>\n",
       "      <th>source_documents_contents_arr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What was the author's experience upon arriving...</td>\n",
       "      <td>The author, Jonathan Harker, arrived in Vienna...</td>\n",
       "      <td>['data_files/Dracula.txt', 'data_files/Dracula...</td>\n",
       "      <td>['How these papers have been placed in sequenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wot kind of experiences can a traveler expect ...</td>\n",
       "      <td>A traveler arriving in London can expect a ric...</td>\n",
       "      <td>['data_files/Dracula.txt', 'data_files/Dracula...</td>\n",
       "      <td>['All day long we seemed to dawdle through a c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What does the Count's letter indicate about th...</td>\n",
       "      <td>The Count's letter indicates that the narrator...</td>\n",
       "      <td>['data_files/Dracula.txt', 'data_files/Dracula...</td>\n",
       "      <td>['The Count halted, putting down my bags, clos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In the context of travel narratives, how is th...</td>\n",
       "      <td>In the context of travel narratives, the term ...</td>\n",
       "      <td>['data_files/Dracula.txt', 'data_files/Dracula...</td>\n",
       "      <td>['When I got on the coach the driver had not t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the significance of the Mittel Land in...</td>\n",
       "      <td>The Mittel Land is significant in the passage ...</td>\n",
       "      <td>['data_files/Dracula.txt', 'data_files/Dracula...</td>\n",
       "      <td>['Beyond the green swelling hills of the Mitte...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query  \\\n",
       "0  What was the author's experience upon arriving...   \n",
       "1  Wot kind of experiences can a traveler expect ...   \n",
       "2  What does the Count's letter indicate about th...   \n",
       "3  In the context of travel narratives, how is th...   \n",
       "4  What is the significance of the Mittel Land in...   \n",
       "\n",
       "                                              result  \\\n",
       "0  The author, Jonathan Harker, arrived in Vienna...   \n",
       "1  A traveler arriving in London can expect a ric...   \n",
       "2  The Count's letter indicates that the narrator...   \n",
       "3  In the context of travel narratives, the term ...   \n",
       "4  The Mittel Land is significant in the passage ...   \n",
       "\n",
       "                          source_documents_names_arr  \\\n",
       "0  ['data_files/Dracula.txt', 'data_files/Dracula...   \n",
       "1  ['data_files/Dracula.txt', 'data_files/Dracula...   \n",
       "2  ['data_files/Dracula.txt', 'data_files/Dracula...   \n",
       "3  ['data_files/Dracula.txt', 'data_files/Dracula...   \n",
       "4  ['data_files/Dracula.txt', 'data_files/Dracula...   \n",
       "\n",
       "                       source_documents_contents_arr  \n",
       "0  ['How these papers have been placed in sequenc...  \n",
       "1  ['All day long we seemed to dawdle through a c...  \n",
       "2  ['The Count halted, putting down my bags, clos...  \n",
       "3  ['When I got on the coach the driver had not t...  \n",
       "4  ['Beyond the green swelling hills of the Mitte...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b012b304",
   "metadata": {},
   "source": [
    "# Context Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "565aea66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute context precision scores\n",
    "precision_score_arr = []\n",
    "for i, row in response_df.iterrows():\n",
    "\n",
    "    # get the relevant contexts for generated response\n",
    "    retrieved_context_arr = ast.literal_eval(row[\"source_documents_contents_arr\"])\n",
    "\n",
    "    # compute context precision score\n",
    "    precision_score = await compute_context_precision_score(\n",
    "        llm_as_judge,\n",
    "        row[\"query\"],\n",
    "        retrieved_context_arr,\n",
    "        row[\"result\"],\n",
    "    )\n",
    "    precision_score_arr.append(precision_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2076692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9999999999, 0.9999999999, 0.6388888888675925, 0.8055555555287036, 0.99999999995, 0.999999999975, 0.999999999975, 0.999999999975, 0.9999999999666667, 0.9166666666361111]\n"
     ]
    }
   ],
   "source": [
    "print(precision_score_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0758f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Context Precision: 0.9361111110674074\n"
     ]
    }
   ],
   "source": [
    "# what is average precision?\n",
    "average_precision = (\n",
    "    sum(precision_score_arr) / len(precision_score_arr) if precision_score_arr else 0\n",
    ")\n",
    "print(f\"Average Context Precision: {average_precision}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a34b13c",
   "metadata": {},
   "source": [
    "# Context Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84780ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute context recall scores\n",
    "recall_score_arr = []\n",
    "for i, row in response_df.iterrows():\n",
    "\n",
    "    # get the relevant contexts for generated response\n",
    "    retrieved_context_arr = ast.literal_eval(row[\"source_documents_contents_arr\"])\n",
    "\n",
    "    # compute context recall score\n",
    "    recall_score = await compute_context_recall_score(\n",
    "        llm_as_judge,\n",
    "        row[\"query\"],\n",
    "        test_data_df.loc[i, \"reference\"],\n",
    "        retrieved_context_arr,\n",
    "    )\n",
    "    recall_score_arr.append(recall_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "159e0860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.8, 0.0, 1.0, 0.5, 0.2857142857142857, 0.5, 0.6666666666666666, 1.0, 0.4]\n"
     ]
    }
   ],
   "source": [
    "print(recall_score_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca5fa146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Context Recall: 0.6152380952380953\n"
     ]
    }
   ],
   "source": [
    "# what is average recall?\n",
    "average_recall = (\n",
    "    sum(recall_score_arr) / len(recall_score_arr) if recall_score_arr else 0\n",
    ")\n",
    "print(f\"Average Context Recall: {average_recall}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f919851d",
   "metadata": {},
   "source": [
    "# Response Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "810b881f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute response relevance scores\n",
    "relevance_score_arr = []\n",
    "for i, row in response_df.iterrows():\n",
    "\n",
    "    # compute response relevance score\n",
    "    relevance_score = await compute_response_relevance_score(\n",
    "        llm_as_judge,\n",
    "        embedding_model,\n",
    "        row[\"query\"],\n",
    "        row[\"result\"],\n",
    "    )\n",
    "    relevance_score_arr.append(relevance_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "76c6df4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[np.float64(0.7258745271466869), np.float64(0.8883869251353049), np.float64(0.9010497172083558), np.float64(0.8846601811379675), np.float64(0.9432960012916567), np.float64(0.9346393395408618), np.float64(0.6710805271893379), np.float64(0.9690563757521421), np.float64(0.6945021232044878), np.float64(0.0)]\n"
     ]
    }
   ],
   "source": [
    "print(relevance_score_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cefdf7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Response Relevance: 0.76125457176068\n"
     ]
    }
   ],
   "source": [
    "# what is average relevance?\n",
    "average_relevance = (\n",
    "    sum(relevance_score_arr) / len(relevance_score_arr) if relevance_score_arr else 0\n",
    ")\n",
    "print(f\"Average Response Relevance: {average_relevance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620f2e82",
   "metadata": {},
   "source": [
    "# Faithfulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "962abb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute faithfulness scores\n",
    "faithfulness_score_arr = []\n",
    "for i, row in response_df.iterrows():\n",
    "\n",
    "    # get the relevant contexts for generated response\n",
    "    retrieved_context_arr = ast.literal_eval(row[\"source_documents_contents_arr\"])\n",
    "\n",
    "    # compute faithfulness score\n",
    "    faithfulness_score = await compute_faithfulness_score(\n",
    "        llm_as_judge,\n",
    "        row[\"query\"],\n",
    "        retrieved_context_arr,\n",
    "        row[\"result\"],\n",
    "    )\n",
    "    faithfulness_score_arr.append(faithfulness_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1fd750a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.06666666666666667, 1.0, 0.9090909090909091, 1.0, 0.6111111111111112, 0.5, 0.6363636363636364, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "print(faithfulness_score_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3c308ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Response Faithfulness: 0.7723232323232323\n"
     ]
    }
   ],
   "source": [
    "# what is average faithfulness?\n",
    "average_faithfulness = (\n",
    "    sum(faithfulness_score_arr) / len(faithfulness_score_arr)\n",
    "    if faithfulness_score_arr\n",
    "    else 0\n",
    ")\n",
    "print(f\"Average Response Faithfulness: {average_faithfulness}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da59e9b",
   "metadata": {},
   "source": [
    "# Factual Correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5e010aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute factual correctness scores\n",
    "factual_correctness_score_arr = []\n",
    "for i, row in response_df.iterrows():\n",
    "\n",
    "    # compute factual correctness score\n",
    "    factual_correctness_score = await compute_factual_correctness_score(\n",
    "        llm_as_judge,\n",
    "        row[\"result\"],\n",
    "        test_data_df.loc[i, \"reference\"],\n",
    "    )\n",
    "    factual_correctness_score_arr.append(factual_correctness_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "02d5b757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[np.float64(0.67), np.float64(0.33), np.float64(0.36), np.float64(0.8), np.float64(0.25), np.float64(0.24), np.float64(0.57), np.float64(0.64), np.float64(0.64), np.float64(0.08)]\n"
     ]
    }
   ],
   "source": [
    "print(factual_correctness_score_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "25fd4e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Response Factual Correctness: 0.458\n"
     ]
    }
   ],
   "source": [
    "# what is average factual correctness score?\n",
    "average_factual_correctness = (\n",
    "    sum(factual_correctness_score_arr) / len(factual_correctness_score_arr)\n",
    "    if factual_correctness_score_arr\n",
    "    else 0\n",
    ")\n",
    "print(f\"Average Response Factual Correctness: {average_factual_correctness}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9e22f6",
   "metadata": {},
   "source": [
    "# Summary of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1b3eb2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create markdown table with variables\n",
    "markdown_text = f\"\"\"\n",
    "|Evaluation Metric| Average Score|\n",
    "|----|----|\n",
    "|Context Precision|{average_precision:.3f}|\n",
    "|Context Recall|{average_recall:.3f}|\n",
    "|Response relevance|{average_relevance:.3f}|\n",
    "|Faithfulness|{average_faithfulness:.3f}|\n",
    "|Factual Correctness|{average_factual_correctness:.3f}|\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "54b68703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "|Evaluation Metric| Average Score|\n",
       "|----|----|\n",
       "|Context Precision|0.936|\n",
       "|Context Recall|0.615|\n",
       "|Response relevance|0.761|\n",
       "|Faithfulness|0.772|\n",
       "|Factual Correctness|0.458|\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(markdown_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f19f5d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragas_eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b411fe2",
   "metadata": {},
   "source": [
    "- Run evaluation metrics on generated test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5aef6564",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import (\n",
    "    compute_context_precision_score,\n",
    "    compute_context_recall_score,\n",
    "    compute_response_relevance_score,\n",
    "    compute_faithfulness_score,\n",
    "    compute_factual_correctness_score,\n",
    ")\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "from ragas.llms.base import llm_factory\n",
    "from IPython.display import Markdown, display\n",
    "import ast\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ada71cbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(\"configs.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3d902f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize LLMs and embedding models\n",
    "llm_as_judge = llm_factory(\"gpt-4o-mini\")\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edd26411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "TEST_DATA_PATH = \"generated_test_data/test_data.csv\"\n",
    "RESPONSE_DATA_DIR = \"generated_responses\"\n",
    "os.makedirs(RESPONSE_DATA_DIR, exist_ok=True)\n",
    "RESPONSE_DATA_PATH = os.path.join(RESPONSE_DATA_DIR, \"rag_responses_v1.csv\")\n",
    "VECTOR_DB_DIR = \"vector_db\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21880637",
   "metadata": {},
   "source": [
    "Read generated test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04ff8c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the generated test data\n",
    "test_data_df = pd.read_csv(TEST_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "939fd57a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63c7f246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference</th>\n",
       "      <th>synthesizer_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who were the Atkinson brothers?</td>\n",
       "      <td>['The Adventure of the Noble Bachelor XI. The ...</td>\n",
       "      <td>The Atkinson brothers were involved in a singu...</td>\n",
       "      <td>single_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who is Mary Jane and what is her significance ...</td>\n",
       "      <td>['You would certainly have been burned, had yo...</td>\n",
       "      <td>Mary Jane is described as incorrigible, and he...</td>\n",
       "      <td>single_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What role does Watson play in the interaction ...</td>\n",
       "      <td>['‘Remarkable as being the scene of the death ...</td>\n",
       "      <td>In the interaction with the Count Von Kramm, W...</td>\n",
       "      <td>single_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How does the context of Bohemia relate to the ...</td>\n",
       "      <td>['to be an immense scandal and seriously compr...</td>\n",
       "      <td>In the narrative, Bohemia is significant as it...</td>\n",
       "      <td>single_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What details can you provide about the locatio...</td>\n",
       "      <td>['the betrothal was publicly proclaimed. That ...</td>\n",
       "      <td>Briony Lodge is described as a bijou villa loc...</td>\n",
       "      <td>single_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0                    Who were the Atkinson brothers?   \n",
       "1  Who is Mary Jane and what is her significance ...   \n",
       "2  What role does Watson play in the interaction ...   \n",
       "3  How does the context of Bohemia relate to the ...   \n",
       "4  What details can you provide about the locatio...   \n",
       "\n",
       "                                  reference_contexts  \\\n",
       "0  ['The Adventure of the Noble Bachelor XI. The ...   \n",
       "1  ['You would certainly have been burned, had yo...   \n",
       "2  ['‘Remarkable as being the scene of the death ...   \n",
       "3  ['to be an immense scandal and seriously compr...   \n",
       "4  ['the betrothal was publicly proclaimed. That ...   \n",
       "\n",
       "                                           reference  \\\n",
       "0  The Atkinson brothers were involved in a singu...   \n",
       "1  Mary Jane is described as incorrigible, and he...   \n",
       "2  In the interaction with the Count Von Kramm, W...   \n",
       "3  In the narrative, Bohemia is significant as it...   \n",
       "4  Briony Lodge is described as a bijou villa loc...   \n",
       "\n",
       "                        synthesizer_name  \n",
       "0  single_hop_specific_query_synthesizer  \n",
       "1  single_hop_specific_query_synthesizer  \n",
       "2  single_hop_specific_query_synthesizer  \n",
       "3  single_hop_specific_query_synthesizer  \n",
       "4  single_hop_specific_query_synthesizer  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b35af81",
   "metadata": {},
   "source": [
    "initialize retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fd16a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load persistent vector DB ===\n",
    "vectordb = Chroma(\n",
    "    persist_directory=VECTOR_DB_DIR,\n",
    "    embedding_function=OpenAIEmbeddings(model=\"text-embedding-3-small\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03aae2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77ef5cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    return_source_documents=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78de8689",
   "metadata": {},
   "source": [
    "generate responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a180b1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO uncomment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd8c5d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response_obj_arr = []\n",
    "# for _, row in test_data_df.iterrows():\n",
    "#     response = qa_chain.invoke(row[\"user_input\"])\n",
    "#     response_obj_arr.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5befefd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save responses to new dataframe using response_obj_arr\n",
    "# response_df = pd.DataFrame(\n",
    "#     {\n",
    "#         \"query\": [resp[\"query\"] for resp in response_obj_arr],\n",
    "#         \"result\": [resp[\"result\"] for resp in response_obj_arr],\n",
    "#         \"source_documents_names_arr\": [\n",
    "#             [doc.metadata[\"source\"] for doc in resp[\"source_documents\"]]\n",
    "#             for resp in response_obj_arr\n",
    "#         ],\n",
    "#         \"source_documents_contents_arr\": [\n",
    "#             [doc.page_content for doc in resp[\"source_documents\"]]\n",
    "#             for resp in response_obj_arr\n",
    "#         ],\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3b68966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response_df.to_csv(RESPONSE_DATA_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d74cd25",
   "metadata": {},
   "source": [
    "evaluate responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e80f0c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_df = pd.read_csv(RESPONSE_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b599daea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>result</th>\n",
       "      <th>source_documents_names_arr</th>\n",
       "      <th>source_documents_contents_arr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who were the Atkinson brothers?</td>\n",
       "      <td>I don't know.</td>\n",
       "      <td>['data_files/Dracula.txt', 'data_files/The Adv...</td>\n",
       "      <td>['in hand, they made light of the attack, and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who is Mary Jane and what is her significance ...</td>\n",
       "      <td>Mary Jane is referred to as the \"incorrigible\"...</td>\n",
       "      <td>['data_files/The Adventures of Sherlock Holmes...</td>\n",
       "      <td>['“Then, how do you know?”\\n\\n“I see it, I ded...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What role does Watson play in the interaction ...</td>\n",
       "      <td>In the interaction with Count Von Kramm, Dr. W...</td>\n",
       "      <td>['data_files/The Adventures of Sherlock Holmes...</td>\n",
       "      <td>['“You had my note?” he asked with a deep hars...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How does the context of Bohemia relate to the ...</td>\n",
       "      <td>The context of Bohemia is significant to the c...</td>\n",
       "      <td>['data_files/The Adventures of Sherlock Holmes...</td>\n",
       "      <td>['“If your Majesty would condescend to state y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What details can you provide about the locatio...</td>\n",
       "      <td>Briony Lodge is described as a bijou villa wit...</td>\n",
       "      <td>['data_files/The Adventures of Sherlock Holmes...</td>\n",
       "      <td>['“I can’t imagine. I suppose that you have be...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query  \\\n",
       "0                    Who were the Atkinson brothers?   \n",
       "1  Who is Mary Jane and what is her significance ...   \n",
       "2  What role does Watson play in the interaction ...   \n",
       "3  How does the context of Bohemia relate to the ...   \n",
       "4  What details can you provide about the locatio...   \n",
       "\n",
       "                                              result  \\\n",
       "0                                      I don't know.   \n",
       "1  Mary Jane is referred to as the \"incorrigible\"...   \n",
       "2  In the interaction with Count Von Kramm, Dr. W...   \n",
       "3  The context of Bohemia is significant to the c...   \n",
       "4  Briony Lodge is described as a bijou villa wit...   \n",
       "\n",
       "                          source_documents_names_arr  \\\n",
       "0  ['data_files/Dracula.txt', 'data_files/The Adv...   \n",
       "1  ['data_files/The Adventures of Sherlock Holmes...   \n",
       "2  ['data_files/The Adventures of Sherlock Holmes...   \n",
       "3  ['data_files/The Adventures of Sherlock Holmes...   \n",
       "4  ['data_files/The Adventures of Sherlock Holmes...   \n",
       "\n",
       "                       source_documents_contents_arr  \n",
       "0  ['in hand, they made light of the attack, and ...  \n",
       "1  ['“Then, how do you know?”\\n\\n“I see it, I ded...  \n",
       "2  ['“You had my note?” he asked with a deep hars...  \n",
       "3  ['“If your Majesty would condescend to state y...  \n",
       "4  ['“I can’t imagine. I suppose that you have be...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b012b304",
   "metadata": {},
   "source": [
    "# Context Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "565aea66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute context precision scores\n",
    "precision_score_arr = []\n",
    "for i, row in response_df.iterrows():\n",
    "\n",
    "    # get the relevant contexts for generated response\n",
    "    retrieved_context_arr = ast.literal_eval(row[\"source_documents_contents_arr\"])\n",
    "\n",
    "    # compute context precision score\n",
    "    precision_score = await compute_context_precision_score(\n",
    "        llm_as_judge,\n",
    "        row[\"query\"],\n",
    "        retrieved_context_arr,\n",
    "        row[\"result\"],\n",
    "    )\n",
    "    precision_score_arr.append(precision_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2076692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.9999999999,\n",
       " 0.99999999995,\n",
       " 0.999999999975,\n",
       " 0.999999999975,\n",
       " 0.999999999975,\n",
       " 0.999999999975,\n",
       " 0.8055555555287036,\n",
       " 0.999999999975,\n",
       " 0.999999999975]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a0758f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Context Precision: 0.8805555555228703\n"
     ]
    }
   ],
   "source": [
    "# what is average precision?\n",
    "average_precision = (\n",
    "    sum(precision_score_arr) / len(precision_score_arr) if precision_score_arr else 0\n",
    ")\n",
    "print(f\"Average Context Precision: {average_precision}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969a709b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a34b13c",
   "metadata": {},
   "source": [
    "# Context Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "84780ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute context recall scores\n",
    "recall_score_arr = []\n",
    "for i, row in response_df.iterrows():\n",
    "\n",
    "    # get the relevant contexts for generated response\n",
    "    retrieved_context_arr = ast.literal_eval(row[\"source_documents_contents_arr\"])\n",
    "\n",
    "    # compute context recall score\n",
    "    recall_score = await compute_context_recall_score(\n",
    "        llm_as_judge,\n",
    "        row[\"query\"],\n",
    "        test_data_df.loc[i, \"reference\"],\n",
    "        retrieved_context_arr,\n",
    "    )\n",
    "    recall_score_arr.append(recall_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "159e0860",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.5, 0.5, 1.0, 1.0, 0.0, 0.3333333333333333, 0.25, 0.75, 0.5]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ca5fa146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Context Recall: 0.4833333333333333\n"
     ]
    }
   ],
   "source": [
    "# what is average recall?\n",
    "average_recall = (\n",
    "    sum(recall_score_arr) / len(recall_score_arr) if recall_score_arr else 0\n",
    ")\n",
    "print(f\"Average Context Recall: {average_recall}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f919851d",
   "metadata": {},
   "source": [
    "# Response Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "810b881f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute response relevance scores\n",
    "relevance_score_arr = []\n",
    "for i, row in response_df.iterrows():\n",
    "\n",
    "    # compute response relevance score\n",
    "    relevance_score = await compute_response_relevance_score(\n",
    "        llm_as_judge,\n",
    "        embedding_model,\n",
    "        row[\"query\"],\n",
    "        row[\"result\"],\n",
    "    )\n",
    "    relevance_score_arr.append(relevance_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "76c6df4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.float64(0.0),\n",
       " np.float64(0.8802317289007987),\n",
       " np.float64(0.9377789431335847),\n",
       " np.float64(0.9119061495065955),\n",
       " np.float64(0.6919304509628469),\n",
       " np.float64(0.6342989958585301),\n",
       " np.float64(0.838835571945377),\n",
       " np.float64(0.8593732347339079),\n",
       " np.float64(0.8937727817588309),\n",
       " np.float64(0.8201806401685484)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevance_score_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cefdf7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Response Relevance: 0.746830849696902\n"
     ]
    }
   ],
   "source": [
    "# what is average relevance?\n",
    "average_relevance = (\n",
    "    sum(relevance_score_arr) / len(relevance_score_arr) if relevance_score_arr else 0\n",
    ")\n",
    "print(f\"Average Response Relevance: {average_relevance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620f2e82",
   "metadata": {},
   "source": [
    "# Faithfulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "962abb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute faithfulness scores\n",
    "faithfulness_score_arr = []\n",
    "for i, row in response_df.iterrows():\n",
    "\n",
    "    # get the relevant contexts for generated response\n",
    "    retrieved_context_arr = ast.literal_eval(row[\"source_documents_contents_arr\"])\n",
    "\n",
    "    # compute faithfulness score\n",
    "    faithfulness_score = await compute_faithfulness_score(\n",
    "        llm_as_judge,\n",
    "        row[\"query\"],\n",
    "        retrieved_context_arr,\n",
    "        row[\"result\"],\n",
    "    )\n",
    "    faithfulness_score_arr.append(faithfulness_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1fd750a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.8, 0.7777777777777778, 0.875, 1.0]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faithfulness_score_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3c308ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Response Faithfulness: 0.8252777777777778\n"
     ]
    }
   ],
   "source": [
    "# what is average faithfulness?\n",
    "average_faithfulness = (\n",
    "    sum(faithfulness_score_arr) / len(faithfulness_score_arr)\n",
    "    if faithfulness_score_arr\n",
    "    else 0\n",
    ")\n",
    "print(f\"Average Response Faithfulness: {average_faithfulness}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da59e9b",
   "metadata": {},
   "source": [
    "# Factual Correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5e010aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute factual correctness scores\n",
    "factual_correctness_score_arr = []\n",
    "for i, row in response_df.iterrows():\n",
    "\n",
    "    # compute factual correctness score\n",
    "    factual_correctness_score = await compute_factual_correctness_score(\n",
    "        llm_as_judge,\n",
    "        row[\"result\"],\n",
    "        test_data_df.loc[i, \"reference\"],\n",
    "    )\n",
    "    factual_correctness_score_arr.append(factual_correctness_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "02d5b757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.float64(0.0),\n",
       " np.float64(0.43),\n",
       " np.float64(0.63),\n",
       " np.float64(0.78),\n",
       " np.float64(0.64),\n",
       " np.float64(0.32),\n",
       " np.float64(0.18),\n",
       " np.float64(0.35),\n",
       " np.float64(0.64),\n",
       " np.float64(0.5)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factual_correctness_score_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "25fd4e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Response Factual Correctness: 0.44700000000000006\n"
     ]
    }
   ],
   "source": [
    "# what is average factual correctness score?\n",
    "average_factual_correctness = (\n",
    "    sum(factual_correctness_score_arr) / len(factual_correctness_score_arr)\n",
    "    if factual_correctness_score_arr\n",
    "    else 0\n",
    ")\n",
    "print(f\"Average Response Factual Correctness: {average_factual_correctness}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9e22f6",
   "metadata": {},
   "source": [
    "# Summary of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1b3eb2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create markdown table with variables\n",
    "markdown_text = f\"\"\"\n",
    "|Evaluation Metric| Average Score|\n",
    "|----|----|\n",
    "|Context Precision|{average_precision:.3f}|\n",
    "|Context Recall|{average_recall:.3f}|\n",
    "|Response relevance|{average_relevance:.3f}|\n",
    "|Faithfulness|{average_faithfulness:.3f}|\n",
    "|Factual Correctness|{average_factual_correctness:.3f}|\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "54b68703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "|Evaluation Metric| Average Score|\n",
       "|----|----|\n",
       "|Context Precision|0.881|\n",
       "|Context Recall|0.483|\n",
       "|Response relevance|0.747|\n",
       "|Faithfulness|0.825|\n",
       "|Factual Correctness|0.447|\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(markdown_text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragas_eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

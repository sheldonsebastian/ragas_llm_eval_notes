{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cfb6ce9",
   "metadata": {},
   "source": [
    "# RAG Evaluation with RAGAS Framework\n",
    "\n",
    "In this notebook, we explore various RAG (Retrieval-Augmented Generation) evaluation metrics using the [RAGAS framework](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/).\n",
    "\n",
    "## Why LLM-as-Judge Evaluation?\n",
    "\n",
    "Traditional testing approaches fail for LLM systems due to non-deterministic outputs and semantic complexity. We use LLMs themselves as judges to evaluate semantic similarity, factual accuracy, and context utilization.\n",
    "\n",
    "## RAG Pipeline Overview\n",
    "\n",
    "RAG enhances language models through a 4-step process:\n",
    "1. **Document Ingestion** → Vector embeddings stored in database\n",
    "2. **Query Processing** → User queries converted to embeddings  \n",
    "3. **Retrieval** → Similar document chunks retrieved\n",
    "4. **Generation** → LLM generates grounded responses using query + context\n",
    "\n",
    "## Evaluation Metrics\n",
    "\n",
    "We examine five key metrics assessing every RAG pipeline component:\n",
    "\n",
    "**Retrieval Quality**\n",
    "\n",
    "1. **Context Precision**: Relevance of retrieved contexts to query\n",
    "2. **Context Recall**: Coverage of all relevant information\n",
    "\n",
    "**Response Quality**\n",
    "\n",
    "3. **Response Relevance**: How well response addresses the query\n",
    "4. **Faithfulness**: Response consistency with retrieved contexts\n",
    "5. **Factual Correctness**: Accuracy of response claims\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7e5923",
   "metadata": {},
   "source": [
    "<image src=\"assets/Evaluation.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18d93c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import SingleTurnSample\n",
    "from ragas.metrics import (\n",
    "    LLMContextPrecisionWithoutReference,\n",
    "    LLMContextRecall,\n",
    "    ResponseRelevancy,\n",
    "    Faithfulness,\n",
    "    FactualCorrectness,\n",
    ")\n",
    "from ragas.llms.base import llm_factory\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c30cb95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load environment variables\n",
    "load_dotenv(\"configs.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c24f634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODELS\n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "LLM_MODEL = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0cf9b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize LLMs and embedding models\n",
    "llm_as_judge = llm_factory(LLM_MODEL)\n",
    "embeddings = OpenAIEmbeddings(model=EMBEDDING_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cd2dbd",
   "metadata": {},
   "source": [
    "## 1. Context Precision\n",
    "\n",
    "Context Precision measures how well a system puts the most useful chunks at the top of your search results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52a48bd",
   "metadata": {},
   "source": [
    "<image src=\"assets/Zoom Context Precision.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326bb358",
   "metadata": {},
   "source": [
    "For example, we have sample data shown below where `user_query` is the user query, `retrieved_contexts` simulates the retrieved text chunks, and `response` is the model generated output. \n",
    "\n",
    "We initialize the context precision metric with our LLM as judge. This metric evaluates how well the most relevant contexts are ranked at the top of the retrieved results, providing a score between 0 and 1 where 1 indicates perfect precision with all relevant contexts ranked highest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0698a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99999999995\n"
     ]
    }
   ],
   "source": [
    "# sample data for context precision\n",
    "sample_data = {\n",
    "    \"user_query\": \"What is the capital of France?\",\n",
    "    \"retrieved_contexts\": [\n",
    "        \"France is a country in Europe. Its capital is Paris.\",\n",
    "        \"The Eiffel Tower is located in Paris, the capital city of France.\",\n",
    "        \"Berlin is the capital of Germany.\",\n",
    "    ],\n",
    "    \"response\": \"The capital of France is Paris.\",\n",
    "}\n",
    "\n",
    "# initialize LLMContextPrecisionWithoutReference\n",
    "context_precision = LLMContextPrecisionWithoutReference(llm=llm_as_judge)\n",
    "sample = SingleTurnSample(\n",
    "    user_input=sample_data[\"user_query\"],\n",
    "    response=sample_data[\"response\"],\n",
    "    retrieved_contexts=sample_data[\"retrieved_contexts\"],\n",
    ")\n",
    "\n",
    "# print context precision score\n",
    "print(await context_precision.single_turn_ascore(sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98995313",
   "metadata": {},
   "source": [
    "If the retrieved contexts are ranked in a different order where the Berlin chunk **(irrelevant information)** is given higher ranking, then the context precision score drops significantly! This demonstrates how context precision specifically measures the ranking quality of retrieved information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c4c320f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5833333333041666\n"
     ]
    }
   ],
   "source": [
    "# Revised sample data with different context ranking\n",
    "sample_data = {\n",
    "    \"user_query\": \"What is the capital of France?\",\n",
    "    \"retrieved_contexts\": [\n",
    "        \"Berlin is the capital of Germany.\",\n",
    "        \"The Eiffel Tower is located in Paris, the capital city of France.\",\n",
    "        \"France is a country in Europe. Its capital is Paris.\",\n",
    "    ],\n",
    "    \"response\": \"The capital of France is Paris.\",\n",
    "}\n",
    "\n",
    "# initialize LLMContextPrecisionWithoutReference\n",
    "context_precision = LLMContextPrecisionWithoutReference(llm=llm_as_judge)\n",
    "sample = SingleTurnSample(\n",
    "    user_input=sample_data[\"user_query\"],\n",
    "    response=sample_data[\"response\"],\n",
    "    retrieved_contexts=sample_data[\"retrieved_contexts\"],\n",
    ")\n",
    "\n",
    "# print context precision score\n",
    "print(await context_precision.single_turn_ascore(sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7eac39",
   "metadata": {},
   "source": [
    "## 2. Context Recall\n",
    "\n",
    "Context Recall measures how well the retrieved contexts cover all the important facts in the ground truth.\n",
    "\n",
    "$$\\text{Context Recall} = \\frac{\\text{Number of facts supported by retrieved contexts}}{\\text{Total number of facts in the ground truth answer}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5e93c1",
   "metadata": {},
   "source": [
    "<image src=\"assets/Zoom Context Recall.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1c5dca",
   "metadata": {},
   "source": [
    "Below are few samples illustrating how **Context Recall** is computed. \n",
    "\n",
    "Note:\n",
    "* `user_input`: The user's query \n",
    "* `retrieved_contexts`: Simulated retrieved text chunks from the database \n",
    "* `reference`: Ground truth answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912a7f8f",
   "metadata": {},
   "source": [
    "### **Example 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f49954b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# create sample data for context recall\n",
    "sample = SingleTurnSample(\n",
    "    user_input=\"Where is Bob's burger located and what is it famous for?\",\n",
    "    retrieved_contexts=[\n",
    "        \"Bob's burger is owned by Bob Belcher and is located in New York City.\",\n",
    "        \"Bob's burger has a new daily burger of the day special.\",\n",
    "    ],\n",
    "    reference=\"Bob's burger is located in New York City. It is famous for its burger of the day specials.\",\n",
    ")\n",
    "\n",
    "# initialize LLMContextRecall\n",
    "context_recall = LLMContextRecall(llm=llm_as_judge)\n",
    "\n",
    "# print context recall score\n",
    "print(await context_recall.single_turn_ascore(sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770f3304",
   "metadata": {},
   "source": [
    "**Analysis for Example 1**\n",
    "\n",
    "* **Ground truth facts:**\n",
    "    1. Bob's burger is located in New York City.\n",
    "    2. Bob's burger is famous for its burger of the day specials.\n",
    "\n",
    "* **Retrieved contexts:**\n",
    "    * \"Bob's burger is owned by Bob Belcher and is located in New York City.\"\n",
    "    * \"Bob's burger has a new daily burger of the day special.\"\n",
    "\n",
    "* **Fact coverage:**\n",
    "    * The **first fact** (\"located in New York City\") is **fully supported**, as the first context explicitly mentions the location.\n",
    "    * The **second fact** (\"famous for its burger of the day specials\") is **fully supported**, because the second context mentions a daily burger of the day special which directly relates to what makes it famous.\n",
    "\n",
    "$$\\text{Context Recall} = \\frac{\\text{Facts supported by context}}{\\text{Total facts in ground truth}} = \\frac{2}{2} = 1.0$$\n",
    "\n",
    "The high score of 1.0 indicates that all ground truth facts are adequately covered by the retrieved contexts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02de87de",
   "metadata": {},
   "source": [
    "### **Example 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e72a080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "# create sample data for context recall\n",
    "sample = SingleTurnSample(\n",
    "    user_input=\"Where is Bob's burger located and what is it famous for?\",\n",
    "    retrieved_contexts=[\n",
    "        \"Bob's burger is owned by Bob Belcher and and is established since 2011.\",\n",
    "        \"Bob's burger has a new daily burger of the day special.\",\n",
    "    ],\n",
    "    reference=\"Bob's burger is located in New York City. It is famous for its burger of the day specials.\",\n",
    ")\n",
    "\n",
    "# initialize LLMContextRecall\n",
    "context_recall = LLMContextRecall(llm=llm_as_judge)\n",
    "\n",
    "# print context recall score\n",
    "print(await context_recall.single_turn_ascore(sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a511715",
   "metadata": {},
   "source": [
    "**Analysis for Example 2**\n",
    "\n",
    "* **Ground truth facts:**\n",
    "  1. Bob's burger is located in New York City.\n",
    "  2. Bob's burger is famous for its burger of the day specials.\n",
    "\n",
    "* **Retrieved contexts:**\n",
    "  * \"Bob's burger is owned by Bob Belcher and is established since 2011.\"\n",
    "  * \"Bob's burger has a new daily burger of the day special.\"\n",
    "\n",
    "* **Fact coverage:**\n",
    "  * The **first fact** (\"located in New York City\") is **not supported**, as neither context mentions the location of Bob's burger.\n",
    "  * The **second fact** (\"famous for its burger of the day specials\") is **supported**, because the second context mentions a daily burger of the day special.\n",
    "\n",
    "$$\\text{Context Recall} = \\frac{\\text{Facts supported by context}}{\\text{Total facts in ground truth}} = \\frac{1}{2} = 0.5$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d3362b",
   "metadata": {},
   "source": [
    "## 3. Response relevance\n",
    "\n",
    "The Response Relevancy metric measures how relevant a generated response is to the user input. Higher scores indicate better alignment with the user input, while lower scores are given if the response is incomplete or includes redundant information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ee2417",
   "metadata": {},
   "source": [
    "<image src=\"assets/Zoom Response Relevance.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85b477bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9164761343745044\n"
     ]
    }
   ],
   "source": [
    "# create sample data for response relevancy\n",
    "sample = SingleTurnSample(\n",
    "    user_input=\"When was the first super bowl?\",\n",
    "    response=\"The first superbowl was held on Jan 15, 1967\",\n",
    ")\n",
    "\n",
    "# initialize ResponseRelevancy\n",
    "scorer = ResponseRelevancy(llm=llm_as_judge, embeddings=embeddings)\n",
    "response_relevance_score = await scorer.single_turn_ascore(sample)\n",
    "\n",
    "# print response relevancy score\n",
    "print(response_relevance_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d48b6be",
   "metadata": {},
   "source": [
    "## 4. Faithfulness\n",
    "\n",
    "The Faithfulness metric measures how factually consistent a response is with the retrieved context, ranging from 0 to 1 with higher scores indicating better consistency and helping identify if the model is hallucinating its generated response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958b2a53",
   "metadata": {},
   "source": [
    "<image src=\"assets/Zoom Faithfulness.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ca7f4c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# create sample data for faithfulness\n",
    "sample = SingleTurnSample(\n",
    "    user_input=\"When was the first super bowl?\",\n",
    "    response=\"The first superbowl was held on Jan 15, 1967\",\n",
    "    retrieved_contexts=[\n",
    "        \"The First AFL–NFL World Championship Game was an American football game played on January 15, 1967, at the Los Angeles Memorial Coliseum in Los Angeles.\"\n",
    "    ],\n",
    ")\n",
    "\n",
    "# initialize Faithfulness metric\n",
    "scorer = Faithfulness(llm=llm_as_judge)\n",
    "faithfulness_score = await scorer.single_turn_ascore(sample)\n",
    "\n",
    "# print faithfulness score\n",
    "print(faithfulness_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7b3587",
   "metadata": {},
   "source": [
    "## 5. Factual Correctness\n",
    "\n",
    "Factual Correctness measures the agreement between a model’s generated response and a reference ground truth for a given question. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34aecac4",
   "metadata": {},
   "source": [
    "<image src=\"assets/Zoom Factual Correctness.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e7847f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n"
     ]
    }
   ],
   "source": [
    "# create sample data for factual correctness\n",
    "sample = SingleTurnSample(\n",
    "    response=\"Bob has 3 kids: Tina, Gene, Louise, and he also has a dog.\",\n",
    "    reference=\"Bob has 3 kids: Tina, Gene, and Louise.\",\n",
    ")\n",
    "\n",
    "# initialize Factual Correctness metric\n",
    "factual_scorer = FactualCorrectness(llm=llm_as_judge)\n",
    "factual_score = await factual_scorer.single_turn_ascore(sample)\n",
    "\n",
    "# print factual correctness score\n",
    "print(factual_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23377782",
   "metadata": {},
   "source": [
    "# Summary of metrics\n",
    "\n",
    "|Metric|Assesses|Ground Truth required|\n",
    "|----|-----|----------|\n",
    "|Context Precision|Retrieval|No|\n",
    "|Context Recall|Retrieval|Yes, ground truth response required.|\n",
    "|Response Relevance|Generation|No|\n",
    "|Faithfulness|Generation|No|\n",
    "|Factual Correctness|Generation|Yes, ground truth response required.|\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragas_eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

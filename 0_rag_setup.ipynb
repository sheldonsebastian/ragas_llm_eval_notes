{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "baa42f3a",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to build a Retrieval Augmented Generation (RAG) system using vector databases. I'll walk through the complete process of setting up a RAG pipeline that ingests classic literature (Dracula and The Adventures of Sherlock Holmes) and enables question-answering capabilities based solely on the content of these books.\n",
    "\n",
    "**What You'll Learn**\n",
    "\n",
    "- How RAG systems work and why they're important for grounding LLM responses\n",
    "- Setting up document ingestion pipelines with text chunking strategies\n",
    "- Creating and querying vector databases using ChromaDB\n",
    "- Combining retrieval with language model generation for accurate, source-backed answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9180f528",
   "metadata": {},
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ac5dbc",
   "metadata": {},
   "source": [
    "### How will LLM know your data?\n",
    "\n",
    "LLMs are trained on publicly available internet data up to their training cutoff date. They have no knowledge of your private documents, proprietary data, or any information that wasn't part of their training dataset. When you ask questions about your specific documents, the LLM cannot provide accurate answers because it simply doesn't have access to that information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7c618e",
   "metadata": {},
   "source": [
    "<img src=\"assets/How would it know your data.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221e9d21",
   "metadata": {},
   "source": [
    "### Manually pass document with query\n",
    "\n",
    "Since model context windows are large, we can provide documents as context to the model and ask questions directly. However, this approach requires manually selecting each document and does not scale effectively when dealing with large numbers of documents or extensive document collections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6ea0dd",
   "metadata": {},
   "source": [
    "<image src = \"assets/Query with document.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3ed320",
   "metadata": {},
   "source": [
    "### Retrieval Augmented Generation\n",
    "\n",
    "Retrieval Augmented Generation (RAG) solves the above problems by following a 4-step process:\n",
    "\n",
    "1. **Document Ingestion**: A collection of documents is passed to the system, where each document is broken down into smaller chunks. This chunking strategy helps provide the model with only relevant content from multiple sections of a document. These chunks are then passed to an embedding model that converts the text into numeric vectors (essentially series of numbers). These numeric vectors are ingested into a dedicated vector database.\n",
    "\n",
    "2. **Query Processing**: When a user asks a query, the query is converted into a numeric representation using the same embedding model and searched within the vector database.\n",
    "\n",
    "3. **Retrieval**: Vectors similar to the user's query are fetched from the database. The embedding model converts these vectors back to their text representation. These relevant text chunks are passed as context along with the original prompt to create the final prompt for the model.\n",
    "\n",
    "4. **Generation**: The model takes the user's original query along with the relevant additional context from the vector database to generate the final response, ensuring the answer is grounded in the provided documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e1f9bc",
   "metadata": {},
   "source": [
    "<image src = \"assets/Retrieval Augmented Generation.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b493780",
   "metadata": {},
   "source": [
    "# Code implementation\n",
    "\n",
    "We now implement RAG using ChromaDB as vector database, OpenAI's text-embedding-3-small as embedding model and GPT-4o-mini as LLM. To create chunks we use LangChain's recursive chunking method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b92633",
   "metadata": {},
   "source": [
    "### Ingest into database\n",
    "\n",
    "This step involves loading text files from the SOURCE_DIR, splitting them into manageable chunks using a recursive text splitter, converting the chunks into vector embeddings using OpenAI's embedding model, and storing the resulting vectors in a persistent ChromaDB database at the DB_DIR path for efficient similarity search and retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf35eab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0220e4c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure the environment variables are loaded\n",
    "load_dotenv(\"configs.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd1e63ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder with .html files\n",
    "SOURCE_DIR = \"data_files\"\n",
    "# Persistent vector database directory\n",
    "DB_DIR = \"vector_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca173fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODELS\n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "LLM_MODEL = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2d829ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      " 50%|█████     | 1/2 [00:02<00:02,  2.46s/it]libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.45s/it]\n"
     ]
    }
   ],
   "source": [
    "loader = DirectoryLoader(SOURCE_DIR, glob=\"**/*.txt\", show_progress=True)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdfeca37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 files loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(docs)} files loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bcf5d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split each book into smaller chunks\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=400)\n",
    "chunks = splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5edca90c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1381 chunks created successfully.\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(chunks)} chunks created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "695b0098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Ingestion complete. Persistent DB stored at: vector_db\n"
     ]
    }
   ],
   "source": [
    "# create folder if it doesnt exist\n",
    "os.makedirs(DB_DIR, exist_ok=True)\n",
    "\n",
    "# vectorize document chunks using text embedding model\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=OpenAIEmbeddings(model=EMBEDDING_MODEL),\n",
    "    persist_directory=DB_DIR,\n",
    ")\n",
    "print(f\"✅ Ingestion complete. Persistent DB stored at: {DB_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2763c72a",
   "metadata": {},
   "source": [
    "### Fetch from database\n",
    "\n",
    "Here we are fetching relevant chunks from database using sample query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7d75721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load persistent vector DB ===\n",
    "vectordb = Chroma(\n",
    "    persist_directory=DB_DIR,\n",
    "    embedding_function=OpenAIEmbeddings(model=EMBEDDING_MODEL),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adf3457d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieval using chromaDB\n",
    "query = \"Who is Irene Adler?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9354277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch top 3 most similar chunks\n",
    "results = vectordb.similarity_search(query, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47b485f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------Result 1-------------------:\n",
      "Source: data_files/The Adventures of Sherlock Holmes.txt\n",
      "Truncated Content: I. A SCANDAL IN BOHEMIA\n",
      "\n",
      "I.\n",
      "\n",
      "To Sherlock Holmes she is always _the_ woman. I have seldom heard him m...\n",
      "\n",
      "-------------------Result 2-------------------:\n",
      "Source: data_files/The Adventures of Sherlock Holmes.txt\n",
      "Truncated Content: “I then lounged down the street and found, as I expected, that there was a mews in a lane which runs...\n",
      "\n",
      "-------------------Result 3-------------------:\n",
      "Source: data_files/The Adventures of Sherlock Holmes.txt\n",
      "Truncated Content: It was close upon four before the door opened, and a drunken-looking groom, ill-kempt and side-whisk...\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"\\n-------------------Result {i}-------------------:\")\n",
    "    print(f\"Source: {doc.metadata.get('source', 'unknown')}\")\n",
    "    print(f\"Truncated Content: {doc.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc6f46b",
   "metadata": {},
   "source": [
    "### Generate LLM response\n",
    "\n",
    "Here we are using the generated vector database along with user query and passing the relevant chunks to LLM to get final sanitized response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "792d5196",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Who is Irene Adler?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fe350c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the LLM and RetrievalQA chain\n",
    "llm = ChatOpenAI(model=LLM_MODEL, temperature=0)\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    return_source_documents=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8760582a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = qa_chain.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f863d495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------Source: data_files/The Adventures of Sherlock Holmes.txt-------------------:\n",
      "Truncated Content: I. A SCANDAL IN BOHEMIA\n",
      "\n",
      "I.\n",
      "\n",
      "To Sherlock Holmes she is always _the_ woman. I have seldom heard him m...\n",
      "\n",
      "-------------------Source: data_files/The Adventures of Sherlock Holmes.txt-------------------:\n",
      "Truncated Content: “I then lounged down the street and found, as I expected, that there was a mews in a lane which runs...\n",
      "\n",
      "-------------------Source: data_files/The Adventures of Sherlock Holmes.txt-------------------:\n",
      "Truncated Content: It was close upon four before the door opened, and a drunken-looking groom, ill-kempt and side-whisk...\n",
      "\n",
      "-------------------Source: data_files/The Adventures of Sherlock Holmes.txt-------------------:\n",
      "Truncated Content: “Mr. Sherlock Holmes, I believe?” said she.\n",
      "\n",
      "“I am Mr. Holmes,” answered my companion, looking at he...\n"
     ]
    }
   ],
   "source": [
    "# print source and first 500 characters of page content\n",
    "for doc in response[\"source_documents\"]:\n",
    "    print(\n",
    "        f\"\\n-------------------Source: {doc.metadata.get('source', 'unknown')}-------------------:\"\n",
    "    )\n",
    "    # preview first 100 chars\n",
    "    print(f\"Truncated Content: {doc.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b62c587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# User Query:\n",
      " Who is Irene Adler?\n",
      "\n",
      "# LLM Response:\n",
      " Irene Adler is a character in Arthur Conan Doyle's story \"A Scandal in Bohemia.\" She is portrayed as a talented and beautiful woman who captivates Sherlock Holmes, who refers to her as \"the woman.\" Adler is known for her intelligence and resourcefulness, and she plays a significant role in the story as she outsmarts Holmes, which is a rare occurrence for the famous detective.\n"
     ]
    }
   ],
   "source": [
    "print(\"# User Query:\\n\", response[\"query\"])\n",
    "print(\"\\n# LLM Response:\\n\", response[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce425f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragas_eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
